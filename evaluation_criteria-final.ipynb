{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d8bbce-40d1-420d-90d5-cdd8c9a7e76b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install google-cloud-aiplatform[evaluation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1e90715-722e-4033-8412-1d95c0d27e4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import vertexai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "e16f0e36-17ac-467e-82b0-6b331684b11e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.evaluation import EvalTask, PointwiseMetric, PointwiseMetricPromptTemplate, PromptTemplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "dd846b1b-2ddd-4051-8f30-a287ff79d916",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define custom evaluation criteria\n",
    "custom_query_quality = PointwiseMetric(\n",
    "    metric=\"custom_query_quality\",\n",
    "    metric_prompt_template=PointwiseMetricPromptTemplate(\n",
    "        criteria={\n",
    "            #\"accuracy\": \"Given the query and the prompt, the answer has to follow the instructions in the query.\",\n",
    "            \"clarity\": \"The output is easy to understand and formatted logically, considering synthetic data.\",\n",
    "            \"relevance\": \"The output aligns with the synthetic context and answers the prompt appropriately.\",\n",
    "            \"insightfulness\": \"The output provides meaningful observations or patterns from the synthetic dataset.\",\n",
    "            \"formatting\": \"The output is well-structured, using tables, lists, or concise sentences where needed.\"\n",
    "        },\n",
    "        rating_rubric={\n",
    "            \"2\": \"The output performs well on all criteria.\",\n",
    "            \"0\": \"The output is somewhat aligned with the criteria.\",\n",
    "            \"-2\": \"The output falls short on all criteria.\"\n",
    "        },\n",
    "        input_variables=[\"prompt\", \"output\"],\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "1b778bb5-7dbe-46c6-a516-60ce85c7fd88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EVAL_DATASET = pd.read_json(\"/home/jupyter/ai-powered-querying-csv-eval/evaluation_dataset_sample.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "cc0bc721-cb71-406e-af7c-1484821d1e01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How many bookings were made by customers in th...</td>\n",
       "      <td>There were 805 bookings made by customers in t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Show the average ticket price for internationa...</td>\n",
       "      <td>{'content': 'Here's the average international ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>How many bookings were made by customers aged ...</td>\n",
       "      <td>There were 188 bookings made by customers aged...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the average ticket price for business ...</td>\n",
       "      <td>The average ticket price for business class bo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many customers booked flights with Delta A...</td>\n",
       "      <td>There were 170 customers who booked flights wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Which age group spends the most on flight tick...</td>\n",
       "      <td>The age group that spends the most on flight t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What percentage of bookings were made through ...</td>\n",
       "      <td>Mobile app bookings accounted for 32.56% of to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the total revenue generated by ticket ...</td>\n",
       "      <td>The total revenue generated from ticket sales ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Which states have the highest number of freque...</td>\n",
       "      <td>{'content': 'Here are the top 5 states with th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How many frequent flyers booked flights to int...</td>\n",
       "      <td>There are 470 frequent flyers who booked fligh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  How many bookings were made by customers in th...   \n",
       "1  Show the average ticket price for internationa...   \n",
       "2  How many bookings were made by customers aged ...   \n",
       "3  What is the average ticket price for business ...   \n",
       "4  How many customers booked flights with Delta A...   \n",
       "5  Which age group spends the most on flight tick...   \n",
       "6  What percentage of bookings were made through ...   \n",
       "7  What is the total revenue generated by ticket ...   \n",
       "8  Which states have the highest number of freque...   \n",
       "9  How many frequent flyers booked flights to int...   \n",
       "\n",
       "                                              output  \n",
       "0  There were 805 bookings made by customers in t...  \n",
       "1  {'content': 'Here's the average international ...  \n",
       "2  There were 188 bookings made by customers aged...  \n",
       "3  The average ticket price for business class bo...  \n",
       "4  There were 170 customers who booked flights wi...  \n",
       "5  The age group that spends the most on flight t...  \n",
       "6  Mobile app bookings accounted for 32.56% of to...  \n",
       "7  The total revenue generated from ticket sales ...  \n",
       "8  {'content': 'Here are the top 5 states with th...  \n",
       "9  There are 470 frequent flyers who booked fligh...  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EVAL_DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "96868ee8-df68-4395-b4da-210e200045f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "evaluation_prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "                You are evaluating the output generated from a synthetic dataset created for testing purposes.\n",
    "                The dataset structure mimics real-world airline bookings but does not contain real data.\n",
    "\n",
    "                ### Evaluation Criteria:\n",
    "                1. **Clarity**: The response is easy to understand and formatted logically.\n",
    "                2. **Relevance**: The response aligns with the synthetic context and answers the query appropriately.\n",
    "                3. **Insightfulness**: The response highlights meaningful patterns or trends in the synthetic data.\n",
    "                4. **Formatting**: The response is well-structured, using tables or concise explanations as needed.\n",
    "\n",
    "                Input Query: {prompt}\n",
    "                Model Output: {output}\n",
    "\n",
    "                Evaluate the response considering the synthetic nature of the data.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Create an evaluation task\n",
    "# Load dataset (CSV or JSON)\n",
    "\n",
    "eval_task = EvalTask(\n",
    "    dataset=EVAL_DATASET,\n",
    "    metrics=[custom_query_quality],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "2e5ea23d-8857-4988-bb38-6ec08c0ade51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from vertexai.generative_models import GenerativeModel\n",
    "\n",
    "model = GenerativeModel(\n",
    "    \"gemini-1.5-flash-002\") #gemini-1.5-flash-002 gemini-1.5-pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "2a6b0aca-c78b-4c17-8f37-c752266eacaa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assembling prompts from the `prompt_template`. The `prompt` column in the `EvalResult.metrics_table` has the assembled prompts used for model response generation.\n",
      "Generating a total of 10 responses from Gemini model gemini-1.5-flash-002.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:03<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 10 responses are successfully generated from Gemini model gemini-1.5-flash-002.\n",
      "Multithreaded Batch Inference took: 3.479947172003449 seconds.\n",
      "Computing metrics with a total of 10 Vertex Gen AI Evaluation Service API requests.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 10/10 [00:17<00:00,  1.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All 10 metric requests are successfully computed.\n",
      "Evaluation Took:17.199717224997585 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Run evaluation for your model\n",
    "eval_result = eval_task.evaluate(\n",
    "    #model=MODEL,  # Replace with your actual tool's evaluation interface\n",
    "    model=model,\n",
    "    prompt_template=evaluation_prompt_template\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "1ae5d52c-215d-467e-8c61-4019acb93a17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>output</th>\n",
       "      <th>response</th>\n",
       "      <th>custom_query_quality/explanation</th>\n",
       "      <th>custom_query_quality/score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>There were 805 bookings made by customers in t...</td>\n",
       "      <td>The model's output is excellent given the cont...</td>\n",
       "      <td>Step 1: Assessment\\nClarity: The response is c...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>{'content': 'Here's the average international ...</td>\n",
       "      <td>The model's output is good and effectively add...</td>\n",
       "      <td>Step 1: Assessment\\nclarity: The output is eas...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>There were 188 bookings made by customers aged...</td>\n",
       "      <td>The model output is excellent given the contex...</td>\n",
       "      <td>Step 1: Assessment\\nClarity: The response is c...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>The average ticket price for business class bo...</td>\n",
       "      <td>The response is satisfactory given the context...</td>\n",
       "      <td>Step 1:\\nClarity: The response is clear and ea...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>There were 170 customers who booked flights wi...</td>\n",
       "      <td>The model output is satisfactory given the con...</td>\n",
       "      <td>Step 1: Assessment\\nClarity: The response is c...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>The age group that spends the most on flight t...</td>\n",
       "      <td>The model's output is good, meeting most of th...</td>\n",
       "      <td>Step 1: Assessment\\nClarity: The response is c...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>Mobile app bookings accounted for 32.56% of to...</td>\n",
       "      <td>The model's response is excellent given the co...</td>\n",
       "      <td>Step 1: Assessment\\nclarity: The response is c...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>The total revenue generated from ticket sales ...</td>\n",
       "      <td>The response is adequate given the context of ...</td>\n",
       "      <td>Step 1: Assessment\\nclarity: The output is cle...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>{'content': 'Here are the top 5 states with th...</td>\n",
       "      <td>The model's response is good, but could be imp...</td>\n",
       "      <td>Step 1: Assessment\\nclarity: The output is eas...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>\\n                You are evaluating the outpu...</td>\n",
       "      <td>There are 470 frequent flyers who booked fligh...</td>\n",
       "      <td>The model's response is satisfactory given the...</td>\n",
       "      <td>Step 1: Assessment:\\nClarity: The response is ...</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt  \\\n",
       "0  \\n                You are evaluating the outpu...   \n",
       "1  \\n                You are evaluating the outpu...   \n",
       "2  \\n                You are evaluating the outpu...   \n",
       "3  \\n                You are evaluating the outpu...   \n",
       "4  \\n                You are evaluating the outpu...   \n",
       "5  \\n                You are evaluating the outpu...   \n",
       "6  \\n                You are evaluating the outpu...   \n",
       "7  \\n                You are evaluating the outpu...   \n",
       "8  \\n                You are evaluating the outpu...   \n",
       "9  \\n                You are evaluating the outpu...   \n",
       "\n",
       "                                              output  \\\n",
       "0  There were 805 bookings made by customers in t...   \n",
       "1  {'content': 'Here's the average international ...   \n",
       "2  There were 188 bookings made by customers aged...   \n",
       "3  The average ticket price for business class bo...   \n",
       "4  There were 170 customers who booked flights wi...   \n",
       "5  The age group that spends the most on flight t...   \n",
       "6  Mobile app bookings accounted for 32.56% of to...   \n",
       "7  The total revenue generated from ticket sales ...   \n",
       "8  {'content': 'Here are the top 5 states with th...   \n",
       "9  There are 470 frequent flyers who booked fligh...   \n",
       "\n",
       "                                            response  \\\n",
       "0  The model's output is excellent given the cont...   \n",
       "1  The model's output is good and effectively add...   \n",
       "2  The model output is excellent given the contex...   \n",
       "3  The response is satisfactory given the context...   \n",
       "4  The model output is satisfactory given the con...   \n",
       "5  The model's output is good, meeting most of th...   \n",
       "6  The model's response is excellent given the co...   \n",
       "7  The response is adequate given the context of ...   \n",
       "8  The model's response is good, but could be imp...   \n",
       "9  The model's response is satisfactory given the...   \n",
       "\n",
       "                    custom_query_quality/explanation  \\\n",
       "0  Step 1: Assessment\\nClarity: The response is c...   \n",
       "1  Step 1: Assessment\\nclarity: The output is eas...   \n",
       "2  Step 1: Assessment\\nClarity: The response is c...   \n",
       "3  Step 1:\\nClarity: The response is clear and ea...   \n",
       "4  Step 1: Assessment\\nClarity: The response is c...   \n",
       "5  Step 1: Assessment\\nClarity: The response is c...   \n",
       "6  Step 1: Assessment\\nclarity: The response is c...   \n",
       "7  Step 1: Assessment\\nclarity: The output is cle...   \n",
       "8  Step 1: Assessment\\nclarity: The output is eas...   \n",
       "9  Step 1: Assessment:\\nClarity: The response is ...   \n",
       "\n",
       "   custom_query_quality/score  \n",
       "0                         2.0  \n",
       "1                         2.0  \n",
       "2                         2.0  \n",
       "3                         0.0  \n",
       "4                         2.0  \n",
       "5                         0.0  \n",
       "6                         2.0  \n",
       "7                         0.0  \n",
       "8                         0.0  \n",
       "9                         2.0  "
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View results\n",
    "eval_result.metrics_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "39e443d7-7e20-4d22-88c8-637fa7b67a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The response is adequate given the limited information provided about the synthetic dataset.  Let's evaluate it against the criteria:\\n\\n1. **Clarity:** The response is perfectly clear and concise.  It directly answers the question.\\n\\n2. **Relevance:** The response is highly relevant. It provides the requested total revenue figure for Delta Airlines ticket sales.\\n\\n3. **Insightfulness:** The response lacks insightfulness. While it provides the answer, it offers no context.  For example, it would be more insightful if it included the number of tickets sold, average ticket price, or a comparison to other airlines in the synthetic dataset.  This is understandable given the limitations of the input – we only asked for a total revenue figure.\\n\\n4. **Formatting:** The formatting is excellent; it's simple, direct, and easy to read.\\n\\n**Overall:**  The response is good in terms of clarity, relevance, and formatting. However,  because it only provides a single number, the insightfulness is limited. To improve, the response should ideally provide more context or related metrics derived from the synthetic dataset.  The evaluation should consider that the lack of additional insight is not necessarily a flaw of the response, but a limitation of the question asked and the information provided. A more insightful answer would require a more complex query.\\n\""
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.metrics_table[\"response\"][7]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "8a6b768e-8cf1-4b9d-a7b4-86a920477669",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Step 1: Clarity: The response is clear and easy to understand. Relevance: The response is relevant to the prompt. Insightfulness: Due to the nature of the prompt, the response lacks insight. It only presents the total revenue and doesn't offer comparisons or trends, which would be more insightful with a synthetic dataset. Formatting: The response is well-formatted.\\nStep 2: Overall, the response effectively addresses the prompt's request, which is to determine the total revenue. However, due to the limitations in the prompt and the lack of additional insights, the response receives a score of 0. It successfully fulfills the basic request but does not provide any further information or analysis that would enhance understanding.\""
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_result.metrics_table[\"custom_query_quality/explanation\"][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "6aa96701-21b5-411b-b6da-88a730af7300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking data saved to eval_file.csv\n"
     ]
    }
   ],
   "source": [
    "# Convert evalresult to DataFrame\n",
    "eval_rslt = pd.DataFrame(eval_result.metrics_table)\n",
    "\n",
    "# Save both datasets to CSV files\n",
    "\n",
    "eval_file = \"eval_file.csv\"\n",
    "\n",
    "\n",
    "eval_rslt.to_csv(eval_file, index=False)\n",
    "\n",
    "\n",
    "print(f\"Booking data saved to {eval_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c725158-88a0-43a8-972e-74508694eed9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
